# Implementation Roadmap

## Overview

This document outlines a phased approach to building the test harness system. Each phase delivers usable functionality while building toward the complete vision.

## Phase 1: Foundation (MVP)

**Goal**: Basic end-to-end flow workingâ€”trigger a test, execute it, see results.

### Components to Build

#### Control Plane (Core)
- [ ] Basic HTTP server with health endpoint
- [ ] gRPC server for agent connections
- [ ] PostgreSQL schema (services, test_runs, test_results, agents)
- [ ] Agent registration and heartbeat handling
- [ ] Simple scheduler (assign work to available agent)
- [ ] Result collection and storage

#### Agent (Core)
- [ ] gRPC client connecting to control plane
- [ ] Registration with capabilities
- [ ] Heartbeat loop
- [ ] Subprocess execution driver
- [ ] Git clone (basic, with token auth)
- [ ] JUnit XML result parser
- [ ] Result reporting

#### Manifest Support
- [ ] YAML manifest parser
- [ ] Basic validation
- [ ] Manual service registration (API endpoint)

#### API
- [ ] POST /api/runs - trigger a test run
- [ ] GET /api/runs - list runs
- [ ] GET /api/runs/{id} - run details with results
- [ ] GET /api/agents - list agents

#### Dashboard (Minimal)
- [ ] Runs list page
- [ ] Run details page (with test results)
- [ ] Agent status page

### Acceptance Criteria
- Can register an agent manually
- Can trigger a test run via API
- Agent clones repo, runs tests, reports results
- Can view results in dashboard

### Estimated Effort
4-6 weeks with 2 developers

---

## Phase 2: Git Integration and Automation

**Goal**: Automatic test discovery and CI integration.

### Components to Build

#### Git Provider Abstraction
- [ ] Git provider interface definition
- [ ] GitHub adapter (API client)
- [ ] Webhook receiver for GitHub
- [ ] File fetching (manifest retrieval)

#### Registry Service
- [ ] Automatic manifest discovery
- [ ] Periodic sync job
- [ ] Webhook-triggered sync
- [ ] Service/test definition storage

#### Enhanced Execution
- [ ] Container execution driver
- [ ] Multiple result format parsers (Jest, Playwright)
- [ ] Artifact collection and storage (S3/MinIO)
- [ ] Test timeouts

#### Dashboard Enhancements
- [ ] Service list view
- [ ] Service detail view with recent runs
- [ ] Artifact viewing (screenshots, logs)
- [ ] Real-time updates (WebSocket)

### Acceptance Criteria
- Push to GitHub triggers test run automatically
- Tests discovered from manifest without manual registration
- Can run containerized tests
- Can view screenshots from failed E2E tests

### Estimated Effort
4-6 weeks with 2 developers

---

## Phase 3: Reliability and Operations

**Goal**: Production-ready with monitoring, notifications, and operational tools.

### Components to Build

#### Notifications
- [ ] Notification rules configuration
- [ ] Slack integration
- [ ] Email integration
- [ ] Webhook integration

#### Agent Management
- [ ] Agent drain functionality
- [ ] Agent capability matching
- [ ] Work cancellation
- [ ] Graceful shutdown handling

#### Observability
- [ ] Prometheus metrics endpoint
- [ ] Structured logging
- [ ] Request tracing
- [ ] Health check improvements

#### Dashboard Enhancements
- [ ] Agent management (drain, status)
- [ ] Notification configuration UI
- [ ] Test run cancellation
- [ ] Rerun functionality

#### Operational Features
- [ ] Scheduled test runs
- [ ] Test retries
- [ ] Allow-failure tests
- [ ] Test dependencies (ordering)

### Acceptance Criteria
- Get Slack notification on test failure
- Can drain agent without losing work
- Metrics visible in Grafana
- Can schedule nightly test runs

### Estimated Effort
4-6 weeks with 2 developers

---

## Phase 4: Analytics and Intelligence

**Goal**: Insights into test health and quality trends.

### Components to Build

#### Analytics
- [ ] Daily stats aggregation job
- [ ] Pass rate trends
- [ ] Duration trends
- [ ] Per-service metrics

#### Flaky Test Detection
- [ ] Flakiness scoring algorithm
- [ ] Flaky test list
- [ ] Quarantine functionality
- [ ] Issue tracking integration

#### Failure Analysis
- [ ] Error signature grouping
- [ ] Failure trends
- [ ] Regression detection

#### Dashboard Enhancements
- [ ] Trends and analytics page
- [ ] Flaky tests page
- [ ] Failure analysis view
- [ ] Service health dashboard

### Acceptance Criteria
- Can see pass rate trend over 30 days
- Flaky tests automatically identified
- Similar failures grouped together
- Can quarantine a flaky test

### Estimated Effort
3-4 weeks with 2 developers

---

## Phase 5: Scale and Polish

**Goal**: Handle larger workloads and improve user experience.

### Components to Build

#### Scale Improvements
- [ ] Control plane horizontal scaling
- [ ] Redis for shared state
- [ ] Agent auto-scaling support
- [ ] Performance optimization

#### Additional Git Providers
- [ ] GitLab adapter
- [ ] Bitbucket adapter (if needed)

#### Advanced Features
- [ ] Test parallelism within a run
- [ ] Partial reruns (failed tests only)
- [ ] Test impact analysis (run affected tests)
- [ ] Custom result formats

#### Dashboard Polish
- [ ] Search functionality
- [ ] Mobile responsiveness
- [ ] Keyboard shortcuts
- [ ] Export/reporting

### Acceptance Criteria
- Can run 100+ concurrent test runs
- Works with GitLab repositories
- Can rerun only failed tests
- Dashboard works on mobile

### Estimated Effort
4-6 weeks with 2 developers

---

## Technical Debt and Ongoing

Throughout all phases, allocate time for:
- [ ] Unit and integration tests for harness itself
- [ ] Documentation
- [ ] Security hardening
- [ ] Dependency updates
- [ ] Performance profiling
- [ ] User feedback incorporation

---

## User Stories by Phase

### Phase 1 User Stories

**US-1.1**: As a developer, I want to trigger a test run via API so that I can test my changes.

**US-1.2**: As a developer, I want to see the status of my test run so that I know if tests passed.

**US-1.3**: As a developer, I want to see individual test results so that I can identify which tests failed.

**US-1.4**: As a platform engineer, I want to deploy an agent so that tests can run in my network.

**US-1.5**: As a platform engineer, I want to see agent status so that I know if agents are healthy.

### Phase 2 User Stories

**US-2.1**: As a developer, I want tests to run automatically when I push to GitHub so that I don't have to trigger manually.

**US-2.2**: As a developer, I want to define my tests in a YAML file so that they're version-controlled with my code.

**US-2.3**: As a developer, I want to see screenshots from failed E2E tests so that I can debug visually.

**US-2.4**: As a developer, I want to run tests in a container so that I have a consistent environment.

**US-2.5**: As a platform engineer, I want the system to discover tests automatically so that I don't have to register them manually.

### Phase 3 User Stories

**US-3.1**: As a developer, I want to receive Slack notifications when my tests fail so that I'm informed quickly.

**US-3.2**: As a developer, I want to rerun a failed test run so that I can verify a fix.

**US-3.3**: As a platform engineer, I want to drain an agent before maintenance so that running tests aren't interrupted.

**US-3.4**: As a platform engineer, I want to see metrics in Grafana so that I can monitor system health.

**US-3.5**: As a developer, I want to schedule nightly test runs so that comprehensive tests run outside work hours.

### Phase 4 User Stories

**US-4.1**: As a manager, I want to see pass rate trends so that I can track quality over time.

**US-4.2**: As a developer, I want to see which tests are flaky so that I can prioritize fixing them.

**US-4.3**: As a developer, I want to quarantine a flaky test so that it doesn't block the pipeline.

**US-4.4**: As a developer, I want to see grouped failures so that I can identify common issues.

### Phase 5 User Stories

**US-5.1**: As a platform engineer, I want to add more agents automatically based on load so that tests run faster.

**US-5.2**: As a developer, I want to rerun only failed tests so that I don't wait for passing tests again.

**US-5.3**: As a developer, I want to use GitLab instead of GitHub so that I can use my preferred platform.

**US-5.4**: As a developer, I want to search for past test runs so that I can find specific results.

---

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| Agent connectivity issues | Robust reconnection logic, comprehensive logging |
| Result format variations | Flexible parsers, fallback to raw output |
| Git provider rate limits | Caching, webhook-driven updates, backoff |
| Large artifacts | Size limits, streaming uploads, retention policies |
| Test environment isolation | Container execution, resource limits |
| Database growth | Retention policies, aggregation, archival |

---

## Success Metrics

| Metric | Target |
|--------|--------|
| Test run trigger to result | < 5 min (for unit tests) |
| System availability | 99.5% |
| Agent utilization | > 70% during peak |
| Dashboard page load | < 2s |
| Time to onboard new service | < 1 hour |
| Developer satisfaction | NPS > 50 |

---

## Dependencies and Prerequisites

Before starting Phase 1:
- [ ] PostgreSQL instance provisioned
- [ ] S3-compatible storage available
- [ ] Network access between control plane and agent locations
- [ ] GitHub App created (or PAT for initial testing)
- [ ] Development environment set up (Go, Node.js)

Before Phase 2:
- [ ] Container registry access
- [ ] GitHub webhook endpoint publicly accessible

Before Phase 3:
- [ ] Slack workspace for notifications
- [ ] Monitoring infrastructure (Prometheus/Grafana)
